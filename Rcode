############################################################
# OFFLINE FITTED Q-EVALUATION WITH TEMPORAL CNN–LSTM
# + SPATIAL GRAPH FEATURES + CATE CLUSTERING
############################################################

# --- Packages ---
pkgs <- c("keras","tensorflow","mclust","cluster","factoextra",
          "dplyr","ggplot2")
for (p in pkgs) if (!requireNamespace(p, quietly=TRUE)) install.packages(p)
library(keras); library(tensorflow)
library(mclust); library(cluster); library(factoextra)
library(dplyr); library(ggplot2)

set.seed(123)

############################################################
# 1. PARAMETERS
############################################################

n <- 1200
T_steps <- 10
p_raw <- 6                # raw covariates
n_actions <- 3
n_rewards <- 2
train_frac <- 0.7
reward_weights <- c(0.6, 0.4)

############################################################
# 2. TEMPORAL COVARIATE SIMULATION  X_i(t)
############################################################

X_base <- matrix(rnorm(n * p_raw), n, p_raw)
X_long <- array(0, dim = c(n, T_steps, p_raw))

for (t in 1:T_steps) {
  X_long[, t, ] <- X_base + 0.2*(t-1) +
    matrix(rnorm(n*p_raw,0,0.15),n,p_raw)
}

############################################################
# 3. SPATIAL GRAPH (DEFINED ONCE, NOT LEARNED)
############################################################

# Past-only historical mean of feature 1
feat1_hist <- apply(X_long[,1:T_steps,1], 1, mean)

dist_mat <- as.matrix(dist(feat1_hist))
k_nn <- 5
nn_idx <- t(apply(dist_mat,1,function(r) order(r)[2:(k_nn+1)]))

# Spatial lag via graph aggregation
X_spatial <- array(0, dim = c(n, T_steps, p_raw))
for (t in 1:T_steps)
  for (j in 1:p_raw)
    for (i in 1:n)
      X_spatial[i,t,j] <- mean(X_long[nn_idx[i,],t,j])

############################################################
# 4. STATE CONSTRUCTION  S_i
#    (NO FUTURE LEAKAGE)
############################################################

# State = concat(temporal mean, last observation)
construct_state <- function(X) {
  X_mean <- apply(X, c(1,3), mean)
  X_last <- X[,T_steps,]
  cbind(X_mean, X_last)
}

S_raw <- construct_state(X_long)
S_spat <- construct_state(X_spatial)
S <- cbind(S_raw, S_spat)
p_state <- ncol(S)

S <- scale(S)
S[!is.finite(S)] <- 0

############################################################
# 5. TREATMENT ASSIGNMENT (OBSERVATIONAL POLICY)
############################################################

x1 <- S[,1]
x2 <- S[,p_raw+1]

logits <- cbind(3*x1, 3*x2, -3*x1)
logits <- logits - apply(logits,1,max)
probs <- exp(logits)/rowSums(exp(logits))

safe_sample <- function(p){
  if(any(!is.finite(p))) p <- rep(1/3,3)
  sample(0:2,1,prob=p)
}
A <- apply(probs,1,safe_sample)

############################################################
# 6. POTENTIAL OUTCOMES WITH HETEROGENEITY
############################################################

base1 <- sin(rowMeans(S[,1:3])) + rnorm(n,0,0.3)
base2 <- cos(rowMeans(S[,4:6])) + rnorm(n,0,0.3)

tau1 <- 1 + 1.5*x2 - 0.5*x1
tau2 <- -1 + 2*x1

Y0 <- cbind(base1, base2)
Y1 <- cbind(base1 + tau1, base2 + tau1)
Y2 <- cbind(base1 + tau2, base2 + tau2)

Y_obs <- matrix(NA,n,n_rewards)
for(i in 1:n){
  if(A[i]==0) Y_obs[i,] <- Y0[i,]
  if(A[i]==1) Y_obs[i,] <- Y1[i,]
  if(A[i]==2) Y_obs[i,] <- Y2[i,]
}

############################################################
# 7. TRAIN / TEST SPLIT
############################################################

idx_tr <- sample(1:n, floor(train_frac*n))
idx_te <- setdiff(1:n, idx_tr)

S_tr <- S[idx_tr,,drop=FALSE]
S_te <- S[idx_te,,drop=FALSE]
A_tr <- A[idx_tr]
Y_tr <- Y_obs[idx_tr,]

############################################################
# 8. CNN–LSTM Q-NETWORK (TEMPORAL ONLY)
############################################################

build_qnet <- function(T,p,a,r,lr=1e-4){
  model <- keras_model_sequential() %>%
    layer_conv_1d(32,3,activation="relu",
                  input_shape=c(T,p),padding="causal") %>%
    layer_lstm(48) %>%
    layer_dense(48,activation="relu") %>%
    layer_dense(a*r)
  
  q_loss <- function(y_true,y_pred){
    yt <- k_reshape(y_true,c(-1,a,r))
    yp <- k_reshape(y_pred,c(-1,a,r))
    w <- k_constant(reward_weights)
    k_mean(k_square(yt-yp)*w)
  }
  
  model %>% compile(optimizer=optimizer_adam(lr),loss=q_loss)
  model
}

############################################################
# 9. OFFLINE FITTED Q-EVALUATION (FQE)
############################################################

train_fqe <- function(X,A,Y,epochs=50,batch=64,
                      importance_sampling=FALSE){
  n <- nrow(X)
  X_arr <- array(X,dim=c(n,1,ncol(X)))
  model <- build_qnet(1,ncol(X),n_actions,n_rewards)
  
  for(ep in 1:epochs){
    idx <- sample(1:n,n,replace=TRUE)
    for(b in split(idx,ceiling(seq_along(idx)/batch))){
      Q <- predict(model,X_arr[b,,,drop=FALSE],verbose=0)
      Qarr <- array(Q,c(length(b),n_actions,n_rewards))
      for(i in seq_along(b))
        Qarr[i,A[b[i]]+1,] <- Y[b[i],]
      model %>% fit(
        X_arr[b,,,drop=FALSE],
        matrix(Qarr,length(b),n_actions*n_rewards),
        epochs=1,verbose=0
      )
    }
    if(ep%%10==0) cat("Epoch",ep,"\n")
  }
  model
}

############################################################
# 10. TRAIN MODEL
############################################################

q_model <- train_fqe(S_tr,A_tr,Y_tr)

############################################################
# 11. Q-VALUES & POLICY
############################################################

Q_te <- predict(q_model,
                array(S_te,c(nrow(S_te),1,ncol(S_te))),
                verbose=0)
Qarr <- array(Q_te,c(nrow(S_te),n_actions,n_rewards))
Qscalar <- apply(Qarr,c(1,2),
                 function(x) sum(x*reward_weights))
pi_hat <- apply(Qscalar,1,which.max)-1

############################################################
# 12. CATE ESTIMATION & CLUSTERING
############################################################

cate_hat <- cbind(
  tau10 = Qscalar[,2]-Qscalar[,1],
  tau20 = Qscalar[,3]-Qscalar[,1]
)

set.seed(42)
k <- 4
km <- kmeans(scale(cate_hat),k,nstart=50)

############################################################
# 13. PCA (VISUALIZATION ONLY)
############################################################

pca <- prcomp(scale(cate_hat))
df <- data.frame(
  PC1=pca$x[,1], PC2=pca$x[,2],
  cluster=factor(km$cluster)
)

ggplot(df,aes(PC1,PC2,color=cluster)) +
  geom_point(size=2,alpha=.8) +
  theme_minimal() +
  labs(title="PCA of Estimated CATEs")

############################################################
# 14. OUTPUT SUMMARY TABLE
############################################################

summary_table <- data.frame(
  
  Quantity = c(
    "Sample size (n)",
    "Time steps (T)",
    "Raw covariates (p_raw)",
    "State dimension (p_state)",
    "Number of actions",
    "Number of rewards",
    "Train sample size",
    "Test sample size",
    "Mean observed reward 1",
    "Mean observed reward 2",
    "SD observed reward 1",
    "SD observed reward 2",
    "Mean Q(s,a=0)",
    "Mean Q(s,a=1)",
    "Mean Q(s,a=2)",
    "Mean CATE (a1 − a0)",
    "Mean CATE (a2 − a0)",
    "Number of CATE clusters"
  ),
  
  Value = c(
    n,
    T_steps,
    p_raw,
    p_state,
    n_actions,
    n_rewards,
    length(idx_tr),
    length(idx_te),
    round(mean(Y_obs[,1]), 3),
    round(mean(Y_obs[,2]), 3),
    round(sd(Y_obs[,1]), 3),
    round(sd(Y_obs[,2]), 3),
    round(mean(Qscalar[,1]), 3),
    round(mean(Qscalar[,2]), 3),
    round(mean(Qscalar[,3]), 3),
    round(mean(cate_hat[,1]), 3),
    round(mean(cate_hat[,2]), 3),
    k
  ),
  
  stringsAsFactors = FALSE
)

summary_table

policy_table <- as.data.frame(table(pi_hat))
colnames(policy_table) <- c("Action", "Count")
policy_table$Proportion <- round(policy_table$Count / sum(policy_table$Count), 3)

policy_table

cluster_table <- as.data.frame(table(km$cluster))
colnames(cluster_table) <- c("Cluster", "Size")
cluster_table$Proportion <- round(cluster_table$Size / sum(cluster_table$Size), 3)

cluster_table

knitr::kable(
  summary_table,
  caption = "Summary of Offline FQE CNN–LSTM Simulation Outputs"
)

knitr::kable(
  policy_table,
  caption = "Learned Policy Distribution (Test Set)"
)

knitr::kable(
  cluster_table,
  caption = "CATE Cluster Sizes"
)

# Oracle CATEs from DGP
cate_oracle <- cbind(
  tau10 = tau1[idx_te],
  tau20 = tau2[idx_te]
)

set.seed(2024)
B <- 500

boot_cate <- function(est, oracle) {
  n <- length(est)  # <- use length for vector
  idx <- sample(seq_len(n), replace = TRUE)
  c(
    est_mean = mean(est[idx]),
    oracle_mean = mean(oracle[idx])
  )
}

boot_tau10 <- replicate(B, boot_cate(cate_hat[,1], cate_oracle[,1]))
boot_tau20 <- replicate(B, boot_cate(cate_hat[,2], cate_oracle[,2]))

# Construct CI table
cate_ci <- data.frame(
  Effect = c("CATE (a1 − a0)", "CATE (a2 − a0)"),
  Est_Mean = c(mean(cate_hat[,1]), mean(cate_hat[,2])),
  Est_L = c(quantile(boot_tau10["est_mean",], .025),
            quantile(boot_tau20["est_mean",], .025)),
  Est_U = c(quantile(boot_tau10["est_mean",], .975),
            quantile(boot_tau20["est_mean",], .975)),
  Oracle_Mean = c(mean(cate_oracle[,1]), mean(cate_oracle[,2])),
  Oracle_L = c(quantile(boot_tau10["oracle_mean",], .025),
               quantile(boot_tau20["oracle_mean",], .025)),
  Oracle_U = c(quantile(boot_tau10["oracle_mean",], .975),
               quantile(boot_tau20["oracle_mean",], .975))
)

cate_ci[, -1] <- round(cate_ci[, -1], 3)
cate_ci

heterogeneity_table <- data.frame(
  Component = c(
    "Dominant policy action",
    "Policy concentration",
    paste0("Cluster ", cluster_table$Cluster)
  ),
  Value = c(
    names(which.max(table(pi_hat))),
    round(max(policy_table$Proportion), 3),
    paste0(
      cluster_table$Size, " (",
      cluster_table$Proportion, ")"
    )
  ),
  stringsAsFactors = FALSE
)

library(kableExtra)

kable(
  cate_ci,
  format = "latex",
  booktabs = TRUE,
  caption = "Oracle vs Estimated CATEs with Bootstrap 95\\% Confidence Intervals",
  col.names = c(
    "Effect",
    "Est. Mean", "Est. 2.5\\%", "Est. 97.5\\%",
    "Oracle Mean", "Oracle 2.5\\%", "Oracle 97.5\\%"
  )
) %>%
  kable_styling(
    latex_options = c("hold_position"),
    font_size = 9
  )


kable(
  heterogeneity_table,
  format = "latex",
  booktabs = TRUE,
  caption = "Policy Concentration and CATE Cluster Distribution"
) %>%
  kable_styling(
    latex_options = c("hold_position"),
    font_size = 9
  )



############################################################
# Real-data CNN–LSTM DQN + CATE Clustering (NYC Flights)
############################################################

# --- Packages ---
needed <- c(
  "keras","tensorflow","nycflights13","dplyr","tidyr","lubridate",
  "purrr","stringr","nnet","mclust","cluster","ggplot2","knitr"
)
for (p in needed) if (!requireNamespace(p, quietly=TRUE)) install.packages(p)
lapply(needed, library, character.only = TRUE)

set.seed(123)

############################################################
# 1. PARAMETERS
############################################################

T_steps <- 10
feat_vars <- c("temp","dewp","humid","wind_dir","wind_speed","wind_gust","precip","pressure","visib")
p <- length(feat_vars)
n_actions <- 3
n_rewards <- 2
train_frac <- 0.7
reward_weights <- c(0.6, 0.4)

############################################################
# 2. WEATHER DATA
############################################################

wx <- nycflights13::weather %>%
  mutate(ts = make_datetime(year, month, day, hour)) %>%
  dplyr::select(c("origin","ts",feat_vars)) %>%
  arrange(origin, ts) %>%
  group_by(origin) %>% fill(all_of(feat_vars), .direction="downup") %>% ungroup()

############################################################
# 3. FLIGHTS DATA + ACTIONS + REWARDS
############################################################

fl <- nycflights13::flights %>%
  mutate(
    sched_dep_hour = floor(sched_dep_time/100),
    ts_dep = make_datetime(year, month, day, sched_dep_hour)
  ) %>%
  filter(origin %in% c("JFK","LGA","EWR")) %>%
  filter(!is.na(dep_delay), !is.na(arr_delay), !is.na(ts_dep)) %>%
  dplyr::select(year, month, day, origin, dest, carrier, flight,
                ts_dep, dep_delay, arr_delay)

# Actions: 0=morning [5-12), 1=afternoon [12-18), 2=evening/night
bucket_of_hour <- function(h){
  if(h>=5 & h<12) return(0L)
  if(h>=12 & h<18) return(1L)
  return(2L)
}
fl <- fl %>% mutate(action = vapply(hour(ts_dep), bucket_of_hour, integer(1)))

# Rewards: negative absolute delays
Y_obs <- cbind(-abs(fl$dep_delay), -abs(fl$arr_delay))

############################################################
# 4. SPATIO-TEMPORAL FEATURES
############################################################

neighbors <- list(JFK=c("LGA","EWR"), LGA=c("JFK","EWR"), EWR=c("JFK","LGA"))

get_seq_row <- function(origin, ts_dep){
  times <- ts_dep - hours(T_steps:1) + hours(1)
  # origin features
  wx_o <- wx %>% filter(origin==!!origin, ts %in% times) %>% arrange(ts)
  if(nrow(wx_o)!=T_steps) return(NULL)
  Xo <- as.matrix(wx_o[, feat_vars])
  # neighbor mean
  nb <- neighbors[[origin]]
  wx_n <- wx %>% filter(origin %in% nb, ts %in% times) %>%
    group_by(ts) %>% summarise(across(all_of(feat_vars), mean, na.rm=TRUE), .groups="drop") %>%
    arrange(ts)
  if(nrow(wx_n)!=T_steps) return(NULL)
  Xn <- as.matrix(wx_n[, feat_vars])
  list(Xo=Xo, Xn=Xn)
}

# Subsample for speed
n_max <- 8000
fl_sub <- fl %>% slice_sample(n=min(n_max,nrow(fl)))
Y_sub <- Y_obs[as.numeric(rownames(fl_sub)), ]

rows <- vector("list", nrow(fl_sub))
keep <- rep(FALSE, nrow(fl_sub))
for(i in seq_len(nrow(fl_sub))){
  gi <- get_seq_row(fl_sub$origin[i], fl_sub$ts_dep[i])
  if(!is.null(gi)){ rows[[i]] <- gi; keep[i] <- TRUE }
}
fl_sub <- fl_sub[keep, ]
Y_sub <- Y_sub[keep, ]
rows <- rows[keep]
n <- nrow(fl_sub)

X_origin <- array(NA_real_, dim=c(n,T_steps,p))
X_spat   <- array(NA_real_, dim=c(n,T_steps,p))
for(i in seq_len(n)){ X_origin[i,,] <- rows[[i]]$Xo; X_spat[i,,] <- rows[[i]]$Xn }

# Combined features
X_combined <- array(NA_real_, dim=c(n,T_steps,2*p))
X_combined[,,1:p] <- X_origin
X_combined[,,(p+1):(2*p)] <- X_spat
p_final <- dim(X_combined)[3]

# Standardization per feature per time-step
for(j in seq_len(p_final)){
  for(t in seq_len(T_steps)){
    v <- X_combined[,t,j]; mu <- mean(v, na.rm=TRUE); sdv <- sd(v, na.rm=TRUE)
    if(!is.finite(sdv) || sdv==0) sdv <- 1
    X_combined[,t,j] <- (v-mu)/sdv
  }
}
X_combined[!is.finite(X_combined)] <- 0

# Actions and rewards
W <- fl_sub$action
Y_obs_train <- Y_sub

# Train/test split
set.seed(123)
idx <- sample(seq_len(n), floor(train_frac*n))
X_train <- X_combined[idx,,]
X_test  <- X_combined[-idx,,]
W_train <- W[idx]; W_test <- W[-idx]
Y_obs_train <- Y_sub[idx,]; Y_obs_test <- Y_sub[-idx,]

############################################################
# 5. CNN-LSTM MODEL
############################################################

build_model <- function(T_steps, p, n_actions, n_rewards, lr=1e-4){
  model <- keras_model_sequential() %>%
    layer_conv_1d(filters=32, kernel_size=3, activation="relu",
                  input_shape=c(T_steps,p), padding="causal") %>%
    layer_lstm(units=48, return_sequences=FALSE) %>%
    layer_dense(units=48, activation="relu") %>%
    layer_dense(units=n_actions*n_rewards, activation="linear")
  
  weighted_mse <- function(y_true, y_pred){
    y_true_r <- k_reshape(y_true, c(-1,n_actions,n_rewards))
    y_pred_r <- k_reshape(y_pred, c(-1,n_actions,n_rewards))
    se <- k_square(y_true_r - y_pred_r)
    w_vec <- k_constant(reward_weights)
    se_w <- se * w_vec
    k_mean(se_w)
  }
  model %>% compile(optimizer=optimizer_adam(lr), loss=weighted_mse)
  model
}

model <- build_model(T_steps, p_final, n_actions, n_rewards)
model2 <- build_model(T_steps, p_final, n_actions, n_rewards)

############################################################
# 6. TRAIN DQN
############################################################

train_dqn <- function(X_train, W_train, Y_train, epochs=40, batch_size=64,
                      weighted_sampling=FALSE, model_init=NULL, verbose_each=10){
  n_train <- dim(X_train)[1]
  if(is.null(model_init)) mod <- build_model(dim(X_train)[2], dim(X_train)[3], n_actions, n_rewards)
  else mod <- model_init
  reward_log <- numeric(epochs)
  
  for(ep in seq_len(epochs)){
    if(weighted_sampling && ep>1){
      # simple uniform weights for now
      sample_idx <- sample(seq_len(n_train), n_train, replace=TRUE)
    } else { sample_idx <- sample(seq_len(n_train), n_train, replace=TRUE) }
    
    # mini-batches
    splits <- split(seq_len(n_train), ceiling(seq_along(seq_len(n_train))/batch_size))
    for(i in seq_along(splits)){
      ids <- splits[[i]]
      Xb <- X_train[ids,,]; Wb <- W_train[ids]; Yb <- Y_train[ids,]
      Xb[!is.finite(Xb)] <- 0; Yb[!is.finite(Yb)] <- 0
      Q_pred <- predict(mod, Xb, verbose=0)
      Qt_array <- array(Q_pred, dim=c(nrow(Xb), n_actions, n_rewards))
      for(ii in seq_len(nrow(Xb))){ act <- Wb[ii]+1; Qt_array[ii,act,] <- Yb[ii,] }
      Q_target_new <- matrix(Qt_array, nrow=nrow(Xb), ncol=n_actions*n_rewards)
      mod %>% fit(x=Xb, y=Q_target_new, epochs=1, verbose=0)
    }
    if(ep %% verbose_each==0) cat("Epoch", ep, "done\n")
  }
  mod
}

# Train models
res_uniform <- train_dqn(X_train, W_train, Y_obs_train, epochs=40, batch_size=64,
                         weighted_sampling=FALSE, model_init=model)
res_weighted <- train_dqn(X_train, W_train, Y_obs_train, epochs=40, batch_size=64,
                          weighted_sampling=TRUE, model_init=model2)

############################################################
# 7. PREDICTION + Q-VALUES
############################################################

q_uni <- predict(res_uniform, X_test, verbose=0)
q_wgt <- predict(res_weighted, X_test, verbose=0)
q_uni_arr <- array(q_uni, dim=c(nrow(X_test), n_actions, n_rewards))
q_wgt_arr <- array(q_wgt, dim=c(nrow(X_test), n_actions, n_rewards))

q_uni_scalar <- apply(q_uni_arr, c(1,2), function(x) sum(x*reward_weights))
q_wgt_scalar <- apply(q_wgt_arr, c(1,2), function(x) sum(x*reward_weights))

pred_action_uni <- apply(q_uni_scalar, 1, which.max)-1
pred_action_wgt <- apply(q_wgt_scalar, 1, which.max)-1

############################################################
# 8. IPS OFF-POLICY EVALUATION
############################################################

flatten_feats <- function(X){ apply(X, c(1,3), mean) }
X_train_flat <- flatten_feats(X_train) %>% as.data.frame()
X_test_flat  <- flatten_feats(X_test)  %>% as.data.frame()
colnames(X_train_flat) <- paste0("f", seq_len(ncol(X_train_flat)))
colnames(X_test_flat)  <- paste0("f", seq_len(ncol(X_test_flat)))

prop_model <- nnet::multinom(factor(W_train) ~ ., data=cbind.data.frame(W_train=factor(W_train), X_train_flat), trace=FALSE)
phat_mat <- predict(prop_model, newdata=X_test_flat, type="probs")
r_test_scalar <- as.numeric(Y_obs_test %*% reward_weights)

policy_value_ips <- function(pred_actions){
  take <- as.integer(pred_actions)+1L
  numer <- ifelse(W_test==pred_actions, r_test_scalar, 0)
  denom <- phat_mat[cbind(seq_along(take), take)]
  mean(numer/pmax(denom,1e-6))
}

val_uni <- policy_value_ips(pred_action_uni)
val_wgt <- policy_value_ips(pred_action_wgt)
val_log <- mean(r_test_scalar)
cat(sprintf("\nIPS uniform: %.4f, IPS weighted: %.4f, logged avg: %.4f\n", val_uni, val_wgt, val_log))

############################################################
# 9. CATE-like contrasts + clustering
############################################################

cate_uni <- cbind(tau10=q_uni_scalar[,2]-q_uni_scalar[,1],
                  tau20=q_uni_scalar[,3]-q_uni_scalar[,1])
cate_wgt <- cbind(tau10=q_wgt_scalar[,2]-q_wgt_scalar[,1],
                  tau20=q_wgt_scalar[,3]-q_wgt_scalar[,1])

set.seed(42)
k <- 4
km_wgt <- kmeans(scale(cate_wgt), centers=k, nstart=50)

df_summary <- data.frame(cluster=km_wgt$cluster,
                         tau10_hat=cate_wgt[,1],
                         tau20_hat=cate_wgt[,2],
                         dep_delay=Y_obs_test[,1],
                         arr_delay=Y_obs_test[,2])

cluster_profiles <- df_summary %>%
  group_by(cluster) %>%
  summarise(
    n=n(),
    mean_tau10=mean(tau10_hat),
    mean_tau20=mean(tau20_hat),
    mean_dep_delay=mean(dep_delay),
    mean_arr_delay=mean(arr_delay)
  )
print(cluster_profiles)

############################################################
# 10. PCA visualization
############################################################

pca <- prcomp(scale(cate_wgt))
pca_df <- data.frame(PC1=pca$x[,1], PC2=pca$x[,2], cluster=factor(km_wgt$cluster))

print(
  ggplot(pca_df, aes(PC1, PC2, color=cluster)) +
    geom_point(alpha=0.8, size=2) +
    labs(title="PCA of estimated CATE-like contrasts (weighted model)") +
    theme_minimal()
)

############################################################
# 11. POLICY DISTRIBUTION TABLE
############################################################

policy_table <- as.data.frame(table(pred_action_wgt))
colnames(policy_table) <- c("Action","Count")
policy_table$Proportion <- round(policy_table$Count / sum(policy_table$Count), 3)

############################################################
# 12. CLUSTER + CATE SUMMARY WITH BOOTSTRAP CIs
############################################################

B <- 500  # bootstrap replicates
set.seed(123)

boot_mean_ci <- function(x, B=500, alpha=0.05){
  n <- length(x)
  boot_means <- replicate(B, mean(sample(x, n, replace=TRUE)))
  c(
    mean = mean(x),
    lower = quantile(boot_means, alpha/2),
    upper = quantile(boot_means, 1-alpha/2)
  )
}

cluster_summary <- df_summary %>%
  group_by(cluster) %>%
  summarise(
    n = n(),
    mean_tau10 = boot_mean_ci(tau10_hat, B=B)[1],
    ci_tau10_lower = boot_mean_ci(tau10_hat, B=B)[2],
    ci_tau10_upper = boot_mean_ci(tau10_hat, B=B)[3],
    mean_tau20 = boot_mean_ci(tau20_hat, B=B)[1],
    ci_tau20_lower = boot_mean_ci(tau20_hat, B=B)[2],
    ci_tau20_upper = boot_mean_ci(tau20_hat, B=B)[3],
    mean_dep_delay = mean(dep_delay),
    mean_arr_delay = mean(arr_delay)
  ) %>%
  ungroup()

############################################################
# 13. MERGE POLICY + CLUSTER TABLES
############################################################

# Create one heterogeneity table for LaTeX
heterogeneity_table <- list(
  "Policy Distribution" = policy_table,
  "Cluster CATE Summary" = cluster_summary
)

# Output to LaTeX using knitr::kable
library(knitr)

cat("\n%--- POLICY DISTRIBUTION ---\n")
kable(policy_table, format="latex", booktabs=TRUE,
      caption="Learned Policy Distribution (Weighted Model)")

cat("\n%--- CLUSTER + CATE SUMMARY ---\n")
kable(cluster_summary, format="latex", booktabs=TRUE,
      caption="Cluster-wise Estimated CATEs with Bootstrap 95% CIs")

